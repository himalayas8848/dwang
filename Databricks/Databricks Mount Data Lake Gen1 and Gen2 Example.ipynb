{"cells":[{"cell_type":"markdown","source":["## Overview\n\nThis notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n\nThis notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."],"metadata":{}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/test.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>No</th><th>day</th><th>hour</th><th>pm25</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>0</td><td>NA</td></tr><tr><td>2</td><td>1</td><td>1</td><td>NA</td></tr><tr><td>3</td><td>1</td><td>2</td><td>NA</td></tr><tr><td>4</td><td>1</td><td>3</td><td>NA</td></tr><tr><td>5</td><td>1</td><td>4</td><td>NA</td></tr><tr><td>6</td><td>1</td><td>5</td><td>NA</td></tr><tr><td>7</td><td>1</td><td>6</td><td>NA</td></tr><tr><td>8</td><td>1</td><td>7</td><td>NA</td></tr><tr><td>9</td><td>1</td><td>8</td><td>NA</td></tr><tr><td>10</td><td>1</td><td>9</td><td>NA</td></tr><tr><td>11</td><td>1</td><td>10</td><td>NA</td></tr><tr><td>12</td><td>1</td><td>11</td><td>NA</td></tr><tr><td>13</td><td>1</td><td>12</td><td>NA</td></tr><tr><td>14</td><td>1</td><td>13</td><td>NA</td></tr><tr><td>15</td><td>1</td><td>14</td><td>NA</td></tr><tr><td>16</td><td>1</td><td>15</td><td>NA</td></tr><tr><td>17</td><td>1</td><td>16</td><td>NA</td></tr><tr><td>18</td><td>1</td><td>17</td><td>NA</td></tr><tr><td>19</td><td>1</td><td>18</td><td>NA</td></tr><tr><td>20</td><td>1</td><td>19</td><td>NA</td></tr><tr><td>21</td><td>1</td><td>20</td><td>NA</td></tr><tr><td>22</td><td>1</td><td>21</td><td>NA</td></tr><tr><td>23</td><td>1</td><td>22</td><td>NA</td></tr><tr><td>24</td><td>1</td><td>23</td><td>NA</td></tr><tr><td>25</td><td>2</td><td>0</td><td>129</td></tr><tr><td>26</td><td>2</td><td>1</td><td>148</td></tr><tr><td>27</td><td>2</td><td>2</td><td>159</td></tr><tr><td>28</td><td>2</td><td>3</td><td>181</td></tr><tr><td>29</td><td>2</td><td>4</td><td>138</td></tr><tr><td>30</td><td>2</td><td>5</td><td>109</td></tr><tr><td>31</td><td>2</td><td>6</td><td>105</td></tr><tr><td>32</td><td>2</td><td>7</td><td>124</td></tr><tr><td>33</td><td>2</td><td>8</td><td>120</td></tr><tr><td>34</td><td>2</td><td>9</td><td>132</td></tr><tr><td>35</td><td>2</td><td>10</td><td>140</td></tr></tbody></table></div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql import *\n\n#username = pp.select('user')[0][0]\n#password = pp.select('password')[0][0]\n\nhost = \"dbcdbserver.database.windows.net\"\nport = 1433\ndatabaseName = \"dbcdb\"\ndbTable = \"SalesLT.Address\"\nproperties = {\n  \"user\": 'dave',\n  \"password\": 'Whh001whh001'\n}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["df = df.limit(5)\n\ndf.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">63</span><span class=\"ansired\">]: </span>5\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["url = \"jdbc:sqlserver://{0}:{1}; database={2}\".format(host, port,databaseName)\n\ndf1 = DataFrameWriter(df)\n\n\ndf1.jdbc(url = url, table='SalesLT.test', mode='overwrite',properties = properties)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["\ndbutils.fs.mkdirs( \"/mnt/flightdata\" )\ndbutils.fs.ls(\"/mnt/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">84</span><span class=\"ansired\">]: </span>\n[FileInfo(path=&apos;dbfs:/mnt/flightdata/&apos;, name=&apos;flightdata/&apos;, size=0),\n FileInfo(path=&apos;dbfs:/mnt/mountdatalake/&apos;, name=&apos;mountdatalake/&apos;, size=0)]\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["dbutils.fs.unmount('/mnt/mountdatalake')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-1314561799416214&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>dbutils<span class=\"ansiyellow\">.</span>fs<span class=\"ansiyellow\">.</span>unmount<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;/mnt/mountdatalake&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/local_disk0/tmp/1566225606220-0/dbutils.py</span> in <span class=\"ansicyan\">f_with_exception_handling</span><span class=\"ansiblue\">(*args, **kwargs)</span>\n<span class=\"ansigreen\">    302</span>                     exc<span class=\"ansiyellow\">.</span>__context__ <span class=\"ansiyellow\">=</span> <span class=\"ansigreen\">None</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    303</span>                     exc<span class=\"ansiyellow\">.</span>__cause__ <span class=\"ansiyellow\">=</span> <span class=\"ansigreen\">None</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 304</span><span class=\"ansiyellow\">                     </span><span class=\"ansigreen\">raise</span> exc<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    305</span>             <span class=\"ansigreen\">return</span> f_with_exception_handling<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    306</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ExecutionError</span>: An error occurred while calling o248.unmount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: Directory not mounted: /mnt/mountdatalake; nested exception is: \n\tjava.lang.IllegalArgumentException: Directory not mounted: /mnt/mountdatalake\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:123)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:63)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.unmount(DBUtilsCore.scala:563)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.unmount(DBUtilsCore.scala:574)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: Directory not mounted: /mnt/mountdatalake\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$deleteMount$1(MetadataManager.scala:192)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:206)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.deleteMount(MetadataManager.scala:181)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:101)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:103)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:102)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:102)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:298)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$3.applyOrElse(DbfsServerBackend.scala:276)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$2(ServerBackend.scala:44)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:61)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:61)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:40)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$4(UsageLogging.scala:415)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:241)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:236)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:233)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:278)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:396)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:339)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:39)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleRPC$2(JettyServer.scala:461)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:461)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:371)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$4(JettyServer.scala:249)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:241)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:236)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:233)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:128)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:278)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:128)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:240)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:156)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:539)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:333)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Data Lake Gen 1 using following codes to mount local drive to data lake\n\nconfigs = {\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\", \n             \"dfs.adls.oauth2.client.id\": \"d13e11b2-110d-401f-9b67-4228e035b124\",\n             \"dfs.adls.oauth2.credential\": \"assVzB3-*Zzm.:4oK5S2p2-you-61Ud6\",\n             \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com/5d75ad06-55a6-415a-b655-50edda524ec6/oauth2/token\"}\n\n## to Data Lake Gen1\ndbutils.fs.mount(source = \"adl://dls3211.azuredatalakestore.net/mountdatalake\", mount_point = \"/mnt/mountdatalake\", extra_configs = configs)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">86</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Data Lake Gen 2 using following codes to mount local drive to data lake\n\nconfigs = {\"fs.azure.account.auth.type\": \"OAuth\",\n       \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n       \"fs.azure.account.oauth2.client.id\": \"d13e11b2-110d-401f-9b67-4228e035b124\",\n       \"fs.azure.account.oauth2.client.secret\": \"assVzB3-*Zzm.:4oK5S2p2-you-61Ud6\",\n       \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/5d75ad06-55a6-415a-b655-50edda524ec6/oauth2/token\",\n       \"fs.azure.createRemoteFileSystemDuringInitialization\": \"true\"}\n\ndbutils.fs.mount(\nsource = \"abfss://placeholder@dlsgen2.dfs.core.windows.net/mountdatalake\",\nmount_point = \"/mnt/flightdata\",\nextra_configs = configs)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">85</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["#pushdown_query = \"(select * from SalesLT.test) as test\"\n#print(pushdown_query)\n\n#Read from SQL database\n#df2 = spark.read.jdbc(url=url, table=pushdown_query, properties=properties)\ndf2 = df\n#Write to a datalake\ndf2.toPandas().to_csv('/dbfs/mnt/mountdatalake/test.csv', header='True', encoding=\"utf-8\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"2019-08-18 - DBFS Example","notebookId":867399730294794},"nbformat":4,"nbformat_minor":0}
